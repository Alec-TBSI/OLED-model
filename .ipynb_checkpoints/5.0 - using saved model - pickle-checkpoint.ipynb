{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared:  0.696328338358\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_excel('C:/Anaconda3/projects/oled/oled.xlsx', sheetname='extend_side')\n",
    "df.dropna(axis=0, inplace=True)\n",
    "\n",
    "X = df[list(df.columns)[1:-2]]\n",
    "y = df[['cd/A']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25, random_state=7)\n",
    "\n",
    "scaler = Normalizer()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "filename = 'rforest_model.sav'\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "print('R-squared: ', loaded_model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 19.587       23.39179487  21.45        20.76333333  23.39179487\n",
      "  26.42179487]\n"
     ]
    }
   ],
   "source": [
    "new_df = pd.read_excel('C:/Anaconda3/projects/oled/oled.xlsx', sheetname='extend_side_sample')\n",
    "new_X = new_df[list(new_df.columns)[:-1]]\n",
    "new_X.head()\n",
    "new_X = scaler.transform(new_X)\n",
    "\n",
    "y_pred = loaded_model.predict(new_X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance 0 prediction: [ 19.587]\n",
      "Instance 1 prediction: [ 23.39179487]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"Instance 0 prediction:\", loaded_model.predict(new_X[0]))\n",
    "print(\"Instance 1 prediction:\", loaded_model.predict(new_X[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree interpreter\n",
    "Using tree interpreter, instruction available on this blog post http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance 0\n",
      "Bias (trainset mean) 24.2759015097\n",
      "Feature contributions:\n",
      "homo2 -4.94\n",
      "tripletdopant 2.86\n",
      "wf -1.78\n",
      "triplet2 -1.41\n",
      "thick 1.33\n",
      "lumo5 -1.0\n",
      "homo_d1 0.85\n",
      "triplet3 -0.73\n",
      "lumo_d1 0.6\n",
      "triplet1 -0.56\n",
      "homo4 -0.42\n",
      "HOMO 0.32\n",
      "homo3 -0.31\n",
      "thick_etl 0.3\n",
      "thick_htl 0.24\n",
      "lumo6 -0.09\n",
      "LUMO 0.05\n",
      "homo1 0.0\n",
      "--------------------\n",
      "Instance 1\n",
      "Bias (trainset mean) 24.2759015097\n",
      "Feature contributions:\n",
      "homo2 -3.56\n",
      "homo1 2.99\n",
      "tripletdopant 2.69\n",
      "thick_etl -2.14\n",
      "thick -1.82\n",
      "homo_d1 1.3\n",
      "homo4 -1.16\n",
      "lumo5 -0.94\n",
      "HOMO 0.84\n",
      "triplet3 -0.6\n",
      "lumo_d1 0.6\n",
      "LUMO 0.56\n",
      "thick_htl 0.43\n",
      "triplet1 0.34\n",
      "triplet2 -0.2\n",
      "lumo6 -0.09\n",
      "wf -0.09\n",
      "homo3 -0.04\n",
      "--------------------\n",
      "Instance 2\n",
      "Bias (trainset mean) 24.2759015097\n",
      "Feature contributions:\n",
      "homo2 -3.56\n",
      "tripletdopant 3.2\n",
      "thick_etl -1.84\n",
      "thick -1.75\n",
      "homo_d1 1.3\n",
      "homo4 -1.16\n",
      "lumo5 -0.94\n",
      "HOMO 0.84\n",
      "lumo_d1 0.6\n",
      "LUMO 0.58\n",
      "lumo6 -0.56\n",
      "thick_htl 0.39\n",
      "triplet1 0.34\n",
      "triplet2 -0.2\n",
      "wf -0.09\n",
      "triplet3 0.07\n",
      "homo1 -0.05\n",
      "homo3 0.0\n",
      "--------------------\n",
      "Instance 3\n",
      "Bias (trainset mean) 24.2759015097\n",
      "Feature contributions:\n",
      "tripletdopant 3.58\n",
      "homo2 -3.41\n",
      "lumo_d1 2.72\n",
      "LUMO -1.98\n",
      "thick -1.82\n",
      "homo4 -1.77\n",
      "thick_etl -1.16\n",
      "HOMO 1.12\n",
      "lumo5 -0.72\n",
      "triplet2 -0.52\n",
      "thick_htl 0.33\n",
      "triplet3 -0.25\n",
      "homo_d1 0.13\n",
      "homo3 0.1\n",
      "wf 0.08\n",
      "homo1 0.07\n",
      "lumo6 -0.01\n",
      "triplet1 0.0\n",
      "--------------------\n",
      "Instance 4\n",
      "Bias (trainset mean) 24.2759015097\n",
      "Feature contributions:\n",
      "homo2 -3.56\n",
      "homo1 2.99\n",
      "tripletdopant 2.69\n",
      "thick_etl -2.14\n",
      "thick -1.82\n",
      "homo_d1 1.3\n",
      "homo4 -1.16\n",
      "lumo5 -0.94\n",
      "HOMO 0.84\n",
      "triplet3 -0.6\n",
      "lumo_d1 0.6\n",
      "LUMO 0.56\n",
      "thick_htl 0.43\n",
      "triplet1 0.34\n",
      "triplet2 -0.2\n",
      "lumo6 -0.09\n",
      "wf -0.09\n",
      "homo3 -0.04\n",
      "--------------------\n",
      "Instance 5\n",
      "Bias (trainset mean) 24.2759015097\n",
      "Feature contributions:\n",
      "homo2 -3.56\n",
      "LUMO 3.07\n",
      "homo1 2.99\n",
      "tripletdopant 2.86\n",
      "thick_etl -1.88\n",
      "thick -1.82\n",
      "homo_d1 1.3\n",
      "homo4 -1.16\n",
      "lumo5 -0.94\n",
      "HOMO 0.84\n",
      "triplet3 -0.6\n",
      "lumo_d1 0.6\n",
      "thick_htl 0.43\n",
      "triplet1 0.34\n",
      "triplet2 -0.27\n",
      "lumo6 -0.09\n",
      "wf 0.09\n",
      "homo3 -0.04\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from treeinterpreter import treeinterpreter as ti\n",
    "\n",
    "prediction, bias, contributions = ti.predict(loaded_model, new_X)\n",
    "names = list(X)\n",
    "for i in range(len(new_X)):\n",
    "    print(\"Instance\", i)\n",
    "    print(\"Bias (trainset mean)\", bias[i])\n",
    "    print(\"Feature contributions:\")\n",
    "    for c, feature in sorted(zip(contributions[i], \n",
    "                                 names), \n",
    "                             key=lambda x: -abs(x[0])):\n",
    "        print(feature, round(c, 2))\n",
    "    print(\"-\"*20 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 19.587       23.39179487  21.45        20.76333333  23.39179487\n",
      "  26.42179487]\n",
      "[ 19.587       23.39179487  21.45        20.76333333  23.39179487\n",
      "  26.42179487]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(prediction)\n",
    "print(bias + np.sum(contributions, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<6x1310 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 461 stored elements in Compressed Sparse Row format>,\n",
       " array([   0,  121,  250,  387,  528,  663,  794,  921, 1058, 1191, 1310], dtype=int32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try using scikit decision path\n",
    "loaded_model.decision_path(new_X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
